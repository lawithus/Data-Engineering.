
ğŸ’³ Credit Scoring, Lending & Fraud Detection System

A full-stack data engineering platform to automate loan application analysis, credit risk scoring, fraud detection, and real-time insights using Kafka, Spark, Flink, PostgreSQL, MongoDB, and Power BI.


ğŸ“Œ Features

- Real-time ingestion with Kafka
- Fraud detection with Flink
- Risk scoring with Spark
- PostgreSQL as data warehouse
- MongoDB for fraud alerts
- Dashboards via Power BI & Grafana
- Access control, governance, and monitoring

ğŸ§± Architecture Diagram


[Kafka Producer] â†’ [Kafka Topic: credit_applications]
        â†“               â†“
 [Flink Fraud Job]   [Spark Risk Job]
        â†“               â†“
  [MongoDB Alerts]   [PostgreSQL DW]
           â†˜           â†™
        [Power BI Dashboards]

âš™ï¸ Components

Kafka Producer
Simulates and streams loan applications.

bash
python ingestion/kafka_producer.py


Flink Job
Flags fraud cases in real time.

Rule: credit_score < 450 && loan_amount > 15000 â†’ alert

Spark Job
Reads valid applications and classifies:
- High Risk: score < 500
- Medium Risk: 500â€“699
- Low Risk: â‰¥ 700

Saves to PostgreSQL.

bash
spark-submit processing/spark_jobs/credit_scoring_job.py

ğŸ§¾ PostgreSQL Tables

- credit_scores: valid risk-classified loans
- data_quality_issues: invalid input logs
- metadata_table: pipeline lineage tracking

Run:
bash
psql -U postgres -d creditdb -f storage/postgres_setup.sql


ğŸƒ MongoDB Setup

js
use fraud_detection;
db.createCollection("fraud_alerts");
db.fraud_alerts.createIndex({ application_id: 1 });


ğŸ“Š Dashboards

Power BI
- Load data from PostgreSQL
- Visuals: risk breakdown, loan approval rates, fraud counts

Grafana (credit_dashboard.json)
- Fraud alerts
- Kafka lag
- Spark job stats
- System metrics via Node Exporter


ğŸ” Governance & Access Control

- PostgreSQL roles & DB grants
- Kafka auth (SSL/SASL configurable)
- Metadata table for lineage
- Logs and quality checks on ingestion


ğŸ“ˆ Monitoring

- Prometheus collects metrics
- Grafana visualizes job and system health

bash
prometheus --config.file=monitoring/prometheus.yml


ğŸ§ª Data Quality Control

Spark filters:
- Missing credit scores
- Invalid loan amounts
- Out-of-range values

Bad records logged in data_quality_issues


ğŸ“¦ Folder Structure

â”œâ”€â”€ ingestion/
â”‚   â””â”€â”€ kafka_producer.py
â”œâ”€â”€ processing/
â”‚   â””â”€â”€ spark_jobs/
â”‚       â””â”€â”€ credit_scoring_job.py
â”‚   â””â”€â”€ flink_jobs/
â”‚       â””â”€â”€ fraud_detection.py
â”œâ”€â”€ storage/
â”‚   â””â”€â”€ postgres_setup.sql
â”‚   â””â”€â”€ mongodb_setup.js
â”œâ”€â”€ monitoring/
â”‚   â””â”€â”€ prometheus.yml
â”‚   â””â”€â”€ credit_dashboard.json
â”œâ”€â”€ dashboards/
â”‚   â””â”€â”€ powerbi_report.pbix
â””â”€â”€ documentation.md


ğŸš€ How to Run End-to-End

1. *Start all services* (Kafka, MongoDB, PostgreSQL, Prometheus)
2. *Start Kafka producer*
3. *Run Spark job*
4. *Open Power BI / Grafana*

---

ğŸ“š Future Enhancements

- Add ML model for dynamic scoring
- REST API for external approval
- Integration with external bureaus
- Kafka + Delta Lake + Iceberg for scalable storage

---

ğŸ‘¤ Maintainer

*Author*: Larry Anesu  
*GitHub*: github.com/larry0003  
*Email*: larâ€¦